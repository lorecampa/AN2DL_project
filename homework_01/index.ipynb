{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import other libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'dataset'\n",
    "dataset = np.load(f'{dataset_dir}/public_data.npz', allow_pickle=True)\n",
    "X = dataset['data']\n",
    "y = dataset['labels']\n",
    "labels = {0:'healthy', 1:'unhealthy'}\n",
    "\n",
    "print(f'Dataset Data Shape {X.shape}, type = {X[0].dtype}')\n",
    "print(f'Dataset Labels Shape {y.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample of images from the training-validation dataset\n",
    "num_img = 10\n",
    "to_show = np.random.randint(0, X.shape[0], num_img);\n",
    "fig, axes = plt.subplots(1, num_img, figsize=(20, 20))\n",
    "\n",
    "# Iterate through the selected number of images\n",
    "for i in range(num_img):\n",
    "    ax = axes[i]\n",
    "    ax.imshow(X[to_show[i]]/255)\n",
    "    ax.set_title('{}'.format(y[to_show[i]]))\n",
    "\n",
    "# Adjust layout and display the images\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the count of occurrences of target classes in the training-validation dataset    \n",
    "print('Counting occurrences of target classes:')\n",
    "print(pd.DataFrame(y, columns=['class'])['class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspection consideration\n",
    "1. Labels are not balanced\n",
    "2. Outliers on the dataset are present\n",
    "3. Labels are not correctly formatted 'healthy' -> 0, 'unhealthy' -> 1\n",
    "4. Dataset is not normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_label_encoder(data, encoding_preferences):\n",
    "    encoded_data = np.array([encoding_preferences.get(val, -1) for val in data])\n",
    "    return encoded_data\n",
    "\n",
    "encoding = {\n",
    "    'healthy': 0,\n",
    "    'unhealthy': 1,\n",
    "}\n",
    "y = custom_label_encoder(y, encoding)\n",
    "\n",
    "print(f'Dataset Labels Shape {y.shape}')\n",
    "# Display the count of occurrences of target classes in the training-validation dataset\n",
    "print('Counting occurrences of target classes:')\n",
    "print(pd.DataFrame(y, columns=['class'])['class'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X / 255).astype('float32')\n",
    "print(f'Dataset Data Shape {X.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_idxs = np.load('outliers_idxs.npy')\n",
    "mask = np.ones(len(X), dtype=bool)\n",
    "mask[outliers_idxs] = False\n",
    "\n",
    "X = X[mask]\n",
    "y = y[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, \n",
    "    y,\n",
    "    test_size=0.1,\n",
    "    stratify=y,\n",
    "    random_state=seed\n",
    ")\n",
    "# Further split the combined training and validation set into a training set and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val,\n",
    "    y_train_val,\n",
    "    test_size = len(X_test), # Ensure validation set size matches test set size\n",
    "    stratify=y_train_val,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "# Print the shapes of the resulting datasets\n",
    "print(\"Training_Validation Data Shape:\", X_train_val.shape)\n",
    "print(\"Training_Validation Label Shape:\", y_train_val.shape)\n",
    "print(\"Train Data Shape:\", X_train.shape)\n",
    "print(\"Train Label Shape:\", y_train.shape)\n",
    "print(\"Validation Data Shape:\", X_val.shape)\n",
    "print(\"Validation Label Shape:\", y_val.shape)\n",
    "print(\"Test Data Shape:\", X_test.shape)\n",
    "print(\"Test Label Shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display the count of occurrences of target classes in the training-validation dataset\n",
    "print('Counting occurrences of y_train classes:')\n",
    "print(pd.DataFrame(y_train, columns=['class'])['class'].value_counts())\n",
    "\n",
    "print('Counting occurrences of y_val classes:')\n",
    "print(pd.DataFrame(y_val, columns=['class'])['class'].value_counts())\n",
    "\n",
    "print('Counting occurrences of y_test classes:')\n",
    "print(pd.DataFrame(y_test, columns=['class'])['class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build quasiVGG9 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_quasiVGG9(input_shape, output_shape):\n",
    "    model = tfk.Sequential(name='quasiVGG9')\n",
    "\n",
    "    model.add(tfkl.Conv2D(32, kernel_size=3, padding='same', activation='relu', input_shape=input_shape, name='conv00'))\n",
    "    model.add(tfkl.Conv2D(32, kernel_size=3, padding='same', activation='relu', name='conv01'))\n",
    "    model.add(tfkl.MaxPooling2D(name='mp0'))\n",
    "\n",
    "    model.add(tfkl.Conv2D(64, kernel_size=3, padding='same', activation='relu', name='conv10'))\n",
    "    model.add(tfkl.Conv2D(64, kernel_size=3, padding='same', activation='relu', name='conv11'))\n",
    "    model.add(tfkl.MaxPooling2D(name='mp1'))\n",
    "\n",
    "    model.add(tfkl.Conv2D(128, kernel_size=3, padding='same', activation='relu', name='conv20'))\n",
    "    model.add(tfkl.Conv2D(128, kernel_size=3, padding='same', activation='relu', name='conv21'))\n",
    "    model.add(tfkl.MaxPooling2D(name='mp2'))\n",
    "\n",
    "    model.add(tfkl.Conv2D(256, kernel_size=3, padding='same', activation='relu', name='conv30'))\n",
    "    model.add(tfkl.Conv2D(256, kernel_size=3, padding='same', activation='relu', name='conv31'))\n",
    "    model.add(tfkl.GlobalAveragePooling2D(name='gap'))\n",
    "\n",
    "    model.add(tfkl.Dense(output_shape, activation='sigmoid', name='Output'))  # Use 'sigmoid' for binary classification\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss=tfk.losses.BinaryCrossentropy(), optimizer=tfk.optimizers.Adam(5e-4), metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key model parameters\n",
    "input_shape = X_train.shape[1:]  # Input shape for the model\n",
    "output_shape = 1  # Output shape for the model\n",
    "batch_size = 128                # Batch size for training\n",
    "epochs = 200                     # Number of training epochs\n",
    "\n",
    "# Print the defined parameters\n",
    "print(\"Epochs:\", epochs)\n",
    "print(\"Batch Size:\", batch_size)\n",
    "print(\"Input Shape:\", input_shape)\n",
    "print(\"Output Shape:\", output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LeNet model and display its summary\n",
    "model = build_quasiVGG9(input_shape, output_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early stopping callback\n",
    "early_stopping = tfk.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, mode='max', restore_best_weights=True)\n",
    "\n",
    "# Train the model and save its history\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping]\n",
    ").history\n",
    "\n",
    "model.save('models/QuasiVGG9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the epoch with the highest validation accuracy\n",
    "best_epoch = np.argmax(history['val_accuracy'])\n",
    "\n",
    "# Plot training and validation performance metrics\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history['loss'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
    "plt.plot(history['val_loss'], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Binary Crossentropy')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# Plot training and validation accuracy, highlighting the best epoch\n",
    "plt.plot(history['accuracy'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
    "plt.plot(history['val_accuracy'], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
    "plt.plot(best_epoch, history['val_accuracy'][best_epoch], marker='*', alpha=0.8, markersize=10, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels for the entire test set\n",
    "predictions = model.predict(X_test, verbose=0)\n",
    "\n",
    "# Display the shape of the predictions\n",
    "print(\"Predictions Shape:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute the confusion matrix\n",
    "rounded_predictions = np.round(predictions.flatten()).astype(int)\n",
    "cm = confusion_matrix(y_test, rounded_predictions)\n",
    "\n",
    "# Compute classification metrics\n",
    "accuracy = accuracy_score(y_test, rounded_predictions)\n",
    "precision = precision_score(y_test, rounded_predictions, average='macro')\n",
    "recall = recall_score(y_test, rounded_predictions, average='macro')\n",
    "f1 = f1_score(y_test, rounded_predictions, average='macro')\n",
    "\n",
    "# Display the computed metrics\n",
    "print('Accuracy:', accuracy.round(4))\n",
    "print('Precision:', precision.round(4))\n",
    "print('Recall:', recall.round(4))\n",
    "print('F1:', f1.round(4))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm.T, xticklabels=list(labels.values()), yticklabels=list(labels.values()), cmap='Blues')\n",
    "plt.xlabel('True labels')\n",
    "plt.ylabel('Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
